{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_model_fune.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPSl5fNdEKtaGtLF9cqfocW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hou-jing/-repository-experiment/blob/main/bert_model_fune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tibobmNjyGiv"
      },
      "source": [
        "import tensorflow as tf\n",
        "train_sentence=['The system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.', 'The <e1>child</e1> was carefully wrapped and bound into the <e2>cradle</e2> by means of a cord.']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce-S4dhjSyaD"
      },
      "source": [
        "train_relation=['Component-Whole', 'Other']"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfNs-TVI3_-9"
      },
      "source": [
        "from tensorflow.keras.preprocessing import text\n",
        "#facts, accu_label, article_label, imprison_label=load_data()\n",
        "somestr = train_sentence\n",
        "tok=text.Tokenizer() #初始化标注器\n",
        "token=tok.fit_on_texts(somestr) #学习出文本的字典\n",
        "word_index = tok.word_index#查看对应的单词和数字的映射关系dict\n",
        "print(word_index)\n",
        "word_docs=tok.word_docs\n",
        "print(word_docs)#document_count, word_counts, word_docs, index_docs, index_word, word_index\n",
        "\n",
        "sequences = tok.texts_to_sequences(somestr) #通过texts_to_sequences 这个dict可以将每个string的每个词转成数字\n",
        "print(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8J5xBUaKvs2"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b98zvslVKvzV",
        "outputId": "68dc0327-72bd-4c81-f279-2b9e8487ed05"
      },
      "source": [
        "def parse_sentence(sentence):\n",
        "  bert_tks=[]\n",
        "\n",
        "  for sentence in sentence:\n",
        "    sentence=sentence.replace('<e1>', '<e1>').replace('</e1>', '<e1>').replace('<e2>', '<e1>').replace('</e2>', '<e1>')\n",
        "    trunks=sentence.split('<e1>')\n",
        "    bert_tokens=[]\n",
        "    bert_tokens.append('[CLS]')\n",
        "    if len(trunks)!=5:\n",
        "      print('error')\n",
        "    else:\n",
        "      for i,trunk in enumerate(trunks):\n",
        "        tks=tokenizer.tokenize(trunk)\n",
        "        # tks=tok.fit_on_texts(trunk)\n",
        "        bert_tokens.extend(tks)\n",
        "        if i==0 or i==1:\n",
        "          bert_tokens.append('[EN1]')\n",
        "        elif i == 2 or i ==3:\n",
        "                bert_tokens.append('[EN2]')\n",
        "\n",
        "    bert_tokens.append(\"[SEP]\")\n",
        "\n",
        "    bert_tks.append(bert_tokens)\n",
        "  return bert_tks\n",
        "bert_token=parse_sentence(train_sentence)\n",
        "print(bert_token)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['[CLS]', 'the', 'system', 'as', 'described', 'above', 'has', 'its', 'greatest', 'application', 'in', 'an', 'array', '##ed', '[EN1]', 'configuration', '[EN1]', 'of', 'antenna', '[EN2]', 'elements', '[EN2]', '.', '[SEP]'], ['[CLS]', 'the', '[EN1]', 'child', '[EN1]', 'was', 'carefully', 'wrapped', 'and', 'bound', 'into', 'the', '[EN2]', 'cradle', '[EN2]', 'by', 'means', 'of', 'a', 'cord', '.', '[SEP]']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD8XYO_bKv6E"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "seq=[tokenizer.convert_tokens_to_ids(toke) for toke in bert_token]\n",
        "print(seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ped0tm8qKv8g",
        "outputId": "0fdacb2c-bcf9-4b5c-c856-2efe5a2e4f5d"
      },
      "source": [
        "train_ids=pad_sequences(sequences=seq,maxlen=128,dtype='long',padding='post')\n",
        "print(train_ids[0])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  101  1996  2291  2004  2649  2682  2038  2049  4602  4646  1999  2019\n",
            "  9140  2098   100  9563   100  1997 13438   100  3787   100  1012   102\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJnT7B7iKv-W",
        "outputId": "b13ced27-4a5c-46d4-faf2-b8378245614c"
      },
      "source": [
        "import numpy as np\n",
        "def build_entitymask(bert_tok):\n",
        "  mask=[[i for i,x in enumerate(msk) if x=='[EN1]' or x=='[EN2]'] for msk in bert_tok]\n",
        "  print(mask)\n",
        "  '''mask=[]\n",
        "  for mak in bert_tok:\n",
        "    for x in range(len(mak)):\n",
        "      if mak[x]=='[EN1]' of msk[x]=='[EN2]':\n",
        "        mask.append(x)\n",
        "        print(mask)'''\n",
        "  e1_masks=[]\n",
        "  e2_masks=[]\n",
        "  for i,mark in enumerate(mask):\n",
        "    e1_mask=np.zeros((128,))\n",
        "    e2_mask=np.zeros((128,))\n",
        "    print(mask[i][0]+1)\n",
        "    print(mask[i][2]+1)\n",
        "    e1_mask[mask[i][0]+1]=1\n",
        "    e2_mask[mask[i][2]+1]=1\n",
        "    e1_masks.append(e1_mask)\n",
        "    e2_masks.append(e2_mask)\n",
        "  return e1_masks,e2_masks\n",
        "train_e1_mask,train_e2_mask=build_entitymask(bert_token)\n",
        "print('训练数据第一个e1的mask{}'.format(train_e1_mask[0]))\n",
        "print('测试数据第一个e2的mask{}'.format(train_e2_mask[0]))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[14, 16, 19, 21], [2, 4, 12, 14]]\n",
            "15\n",
            "20\n",
            "3\n",
            "13\n",
            "训练数据第一个e1的mask[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "测试数据第一个e2的mask[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocNEb67LnGSb",
        "outputId": "30087540-a571-407c-b6c8-faf177b5ecd6"
      },
      "source": [
        "from sklearn.preprocessing.label import LabelEncoder\n",
        "label=LabelEncoder()\n",
        "label_rel=label.fit_transform(train_relation)#fit转为集合，transform转为编码\n",
        "print(label_rel)\n",
        "#查看编码对应的原始值\n",
        "rel=label.inverse_transform([0,1])\n",
        "print(rel)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n",
            "['Component-Whole' 'Other']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vHsp42AqdjH",
        "outputId": "4b024287-faa3-4769-e0ec-49ee86f6b509"
      },
      "source": [
        "#注意力mask\n",
        "train_attention_mask=[[float(i>0) for i in ii] for ii in train_ids]\n",
        "print (train_attention_mask[0])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0SKpcB4Vzlm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEIn4MK4VzsN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EhFtFu3Vzth"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZGPEj26Vzw0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peivph6iVzzb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7Zu9_EuIynF",
        "outputId": "76f16b42-ac07-4b6d-f198-5d949e205371"
      },
      "source": [
        "!pip install bert-tensorflow\n",
        "!pip install tensorflow-hub\n",
        "from bert import tokenization\n",
        "\n",
        "                                                "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-tensorflow\n",
            "  Downloading bert_tensorflow-1.0.4-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 51 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 61 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 64 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow) (1.15.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.4\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNG8pBfkJzLH"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "with tf.Graph().as_default():\n",
        "  bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "  tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "  vocab_file, do_lower_case =tokenization_info[\"vocab_file\"],tokenization_info[\"do_lower_case\"]\n",
        "Fulltoken=tokenization.FullTokenizer(vocab_file=vocab_file,do_lower_case=True)\n",
        "token=Fulltoken.tokenize(train_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNZNA96_4vck",
        "outputId": "f1983efb-00cf-4baf-9b59-1a61e9619425"
      },
      "source": [
        "import gensim\n",
        "text='The system as described configuration'\n",
        "token=gensim.utils.tokenize(text,lower=True)\n",
        "print(token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object simple_tokenize at 0x7f909916f9d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nofyedzo7URY",
        "outputId": "7cc0ae7b-6638-47c3-a800-8bdcbf2494b9"
      },
      "source": [
        "from tensorflow.keras.preprocessing import text\n",
        "#facts, accu_label, article_label, imprison_label=load_data()\n",
        "somestr = ['ha ha gua angry','howa ha gua excited naive']\n",
        "tok=text.Tokenizer() #初始化标注器\n",
        "tok.fit_on_texts(somestr) #学习出文本的字典\n",
        "word_index = tok.word_index#查看对应的单词和数字的映射关系dict\n",
        "print(word_index)\n",
        "sequences = tok.texts_to_sequences(somestr) #通过texts_to_sequences 这个dict可以将每个string的每个词转成数字\n",
        "print(sequences)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ha': 1, 'gua': 2, 'angry': 3, 'howa': 4, 'excited': 5, 'naive': 6}\n",
            "[[1, 1, 2, 3], [4, 1, 2, 5, 6]]\n"
          ]
        }
      ]
    }
  ]
}